{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'font.size': 10})","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-01T16:41:30.657123Z","iopub.execute_input":"2023-06-01T16:41:30.657505Z","iopub.status.idle":"2023-06-01T16:41:30.688078Z","shell.execute_reply.started":"2023-06-01T16:41:30.657476Z","shell.execute_reply":"2023-06-01T16:41:30.687239Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/morticd10/codes.csv')\n\n#structuring\ndf.columns = ['prefix_codes', 'decimal', 'codes', 'description', 'long_description', 'label']","metadata":{"execution":{"iopub.status.busy":"2023-06-01T16:41:33.341228Z","iopub.execute_input":"2023-06-01T16:41:33.341615Z","iopub.status.idle":"2023-06-01T16:41:33.898374Z","shell.execute_reply.started":"2023-06-01T16:41:33.341581Z","shell.execute_reply":"2023-06-01T16:41:33.897068Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-01T16:41:41.798401Z","iopub.execute_input":"2023-06-01T16:41:41.799038Z","iopub.status.idle":"2023-06-01T16:41:41.826205Z","shell.execute_reply.started":"2023-06-01T16:41:41.799007Z","shell.execute_reply":"2023-06-01T16:41:41.825290Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"  prefix_codes decimal  codes  \\\n0          A00       1   A001   \n1          A00       9   A009   \n2         A010       0  A0100   \n3         A010       1  A0101   \n4         A010       2  A0102   \n\n                                       description  \\\n0  Cholera due to Vibrio cholerae 01, biovar eltor   \n1                             Cholera, unspecified   \n2                       Typhoid fever, unspecified   \n3                               Typhoid meningitis   \n4             Typhoid fever with heart involvement   \n\n                                  long_description          label  \n0  Cholera due to Vibrio cholerae 01, biovar eltor        Cholera  \n1                             Cholera, unspecified        Cholera  \n2                       Typhoid fever, unspecified  Typhoid fever  \n3                               Typhoid meningitis  Typhoid fever  \n4             Typhoid fever with heart involvement  Typhoid fever  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prefix_codes</th>\n      <th>decimal</th>\n      <th>codes</th>\n      <th>description</th>\n      <th>long_description</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A00</td>\n      <td>1</td>\n      <td>A001</td>\n      <td>Cholera due to Vibrio cholerae 01, biovar eltor</td>\n      <td>Cholera due to Vibrio cholerae 01, biovar eltor</td>\n      <td>Cholera</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A00</td>\n      <td>9</td>\n      <td>A009</td>\n      <td>Cholera, unspecified</td>\n      <td>Cholera, unspecified</td>\n      <td>Cholera</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A010</td>\n      <td>0</td>\n      <td>A0100</td>\n      <td>Typhoid fever, unspecified</td>\n      <td>Typhoid fever, unspecified</td>\n      <td>Typhoid fever</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A010</td>\n      <td>1</td>\n      <td>A0101</td>\n      <td>Typhoid meningitis</td>\n      <td>Typhoid meningitis</td>\n      <td>Typhoid fever</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A010</td>\n      <td>2</td>\n      <td>A0102</td>\n      <td>Typhoid fever with heart involvement</td>\n      <td>Typhoid fever with heart involvement</td>\n      <td>Typhoid fever</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"len(df.prefix_codes.unique()), len(df)","metadata":{"execution":{"iopub.status.busy":"2023-06-01T16:41:54.745701Z","iopub.execute_input":"2023-06-01T16:41:54.746058Z","iopub.status.idle":"2023-06-01T16:41:54.763969Z","shell.execute_reply.started":"2023-06-01T16:41:54.746032Z","shell.execute_reply":"2023-06-01T16:41:54.761904Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(19927, 71703)"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForSequenceClassification\nfrom torch.utils.data import DataLoader\nfrom transformers import AdamW\nimport torch\n","metadata":{"execution":{"iopub.status.busy":"2023-06-01T16:42:53.771279Z","iopub.execute_input":"2023-06-01T16:42:53.771652Z","iopub.status.idle":"2023-06-01T16:43:07.297152Z","shell.execute_reply.started":"2023-06-01T16:42:53.771626Z","shell.execute_reply":"2023-06-01T16:43:07.296143Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2023-06-01T16:43:07.298964Z","iopub.execute_input":"2023-06-01T16:43:07.299273Z","iopub.status.idle":"2023-06-01T16:43:13.234676Z","shell.execute_reply.started":"2023-06-01T16:43:07.299246Z","shell.execute_reply":"2023-06-01T16:43:13.233632Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cf851acbf1e43759afa68d27a2e4480"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bee808469c5f4f4e94752c5db0bc4531"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"332c34d56577463a8c8b61da56854399"}},"metadata":{}}]},{"cell_type":"code","source":"new_df = df.sample(frac=1, random_state=23)\nprint(f\"Size: {len(new_df)}\")\nnew_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-01T16:43:50.965504Z","iopub.execute_input":"2023-06-01T16:43:50.965931Z","iopub.status.idle":"2023-06-01T16:43:51.014090Z","shell.execute_reply.started":"2023-06-01T16:43:50.965891Z","shell.execute_reply":"2023-06-01T16:43:51.012981Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Size: 71703\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"      prefix_codes decimal    codes  \\\n13730        M2481       9   M24819   \n22501         R197     NaN     R197   \n20893         O891     NaN     O891   \n13975         M260       3    M2603   \n36830       S59032       S  S59032S   \n\n                                             description  \\\n13730  Oth specific joint derangements of unsp should...   \n22501                              Diarrhea, unspecified   \n20893  Cardiac complications of anesthesia during the...   \n13975                             Mandibular hyperplasia   \n36830  Sltr-haris Type III physl fx lower end ulna, l...   \n\n                                        long_description  \\\n13730  Other specific joint derangements of unspecifi...   \n22501                              Diarrhea, unspecified   \n20893  Cardiac complications of anesthesia during the...   \n13975                             Mandibular hyperplasia   \n36830  Salter-Harris Type III physeal fracture of low...   \n\n                                                   label  \n13730  Other specific joint derangements of shoulder,...  \n22501                              Diarrhea, unspecified  \n20893  Cardiac complications of anesthesia during the...  \n13975                        Major anomalies of jaw size  \n36830  Salter-Harris Type III physeal fracture of low...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prefix_codes</th>\n      <th>decimal</th>\n      <th>codes</th>\n      <th>description</th>\n      <th>long_description</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>13730</th>\n      <td>M2481</td>\n      <td>9</td>\n      <td>M24819</td>\n      <td>Oth specific joint derangements of unsp should...</td>\n      <td>Other specific joint derangements of unspecifi...</td>\n      <td>Other specific joint derangements of shoulder,...</td>\n    </tr>\n    <tr>\n      <th>22501</th>\n      <td>R197</td>\n      <td>NaN</td>\n      <td>R197</td>\n      <td>Diarrhea, unspecified</td>\n      <td>Diarrhea, unspecified</td>\n      <td>Diarrhea, unspecified</td>\n    </tr>\n    <tr>\n      <th>20893</th>\n      <td>O891</td>\n      <td>NaN</td>\n      <td>O891</td>\n      <td>Cardiac complications of anesthesia during the...</td>\n      <td>Cardiac complications of anesthesia during the...</td>\n      <td>Cardiac complications of anesthesia during the...</td>\n    </tr>\n    <tr>\n      <th>13975</th>\n      <td>M260</td>\n      <td>3</td>\n      <td>M2603</td>\n      <td>Mandibular hyperplasia</td>\n      <td>Mandibular hyperplasia</td>\n      <td>Major anomalies of jaw size</td>\n    </tr>\n    <tr>\n      <th>36830</th>\n      <td>S59032</td>\n      <td>S</td>\n      <td>S59032S</td>\n      <td>Sltr-haris Type III physl fx lower end ulna, l...</td>\n      <td>Salter-Harris Type III physeal fracture of low...</td>\n      <td>Salter-Harris Type III physeal fracture of low...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def normalise_text (text):\n    text = text.lower() # lowercase\n    text = text.replace(r\"\\#\",\"\") # replaces hashtags\n    text = text.replace(r\"@\",\"\")\n    text = text.replace(r\"[^A-Za-z0-9()!?\\'\\`\\\"]\", \" \")\n    text = text.replace(\"\\s{2,}\", \" \")\n    return text\n\nnew_df['long_description'] = new_df['long_description'].apply(normalise_text)\nnew_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-01T16:43:53.988763Z","iopub.execute_input":"2023-06-01T16:43:53.989147Z","iopub.status.idle":"2023-06-01T16:43:54.117272Z","shell.execute_reply.started":"2023-06-01T16:43:53.989118Z","shell.execute_reply":"2023-06-01T16:43:54.116255Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"      prefix_codes decimal    codes  \\\n13730        M2481       9   M24819   \n22501         R197     NaN     R197   \n20893         O891     NaN     O891   \n13975         M260       3    M2603   \n36830       S59032       S  S59032S   \n\n                                             description  \\\n13730  Oth specific joint derangements of unsp should...   \n22501                              Diarrhea, unspecified   \n20893  Cardiac complications of anesthesia during the...   \n13975                             Mandibular hyperplasia   \n36830  Sltr-haris Type III physl fx lower end ulna, l...   \n\n                                        long_description  \\\n13730  other specific joint derangements of unspecifi...   \n22501                              diarrhea, unspecified   \n20893  cardiac complications of anesthesia during the...   \n13975                             mandibular hyperplasia   \n36830  salter-harris type iii physeal fracture of low...   \n\n                                                   label  \n13730  Other specific joint derangements of shoulder,...  \n22501                              Diarrhea, unspecified  \n20893  Cardiac complications of anesthesia during the...  \n13975                        Major anomalies of jaw size  \n36830  Salter-Harris Type III physeal fracture of low...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prefix_codes</th>\n      <th>decimal</th>\n      <th>codes</th>\n      <th>description</th>\n      <th>long_description</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>13730</th>\n      <td>M2481</td>\n      <td>9</td>\n      <td>M24819</td>\n      <td>Oth specific joint derangements of unsp should...</td>\n      <td>other specific joint derangements of unspecifi...</td>\n      <td>Other specific joint derangements of shoulder,...</td>\n    </tr>\n    <tr>\n      <th>22501</th>\n      <td>R197</td>\n      <td>NaN</td>\n      <td>R197</td>\n      <td>Diarrhea, unspecified</td>\n      <td>diarrhea, unspecified</td>\n      <td>Diarrhea, unspecified</td>\n    </tr>\n    <tr>\n      <th>20893</th>\n      <td>O891</td>\n      <td>NaN</td>\n      <td>O891</td>\n      <td>Cardiac complications of anesthesia during the...</td>\n      <td>cardiac complications of anesthesia during the...</td>\n      <td>Cardiac complications of anesthesia during the...</td>\n    </tr>\n    <tr>\n      <th>13975</th>\n      <td>M260</td>\n      <td>3</td>\n      <td>M2603</td>\n      <td>Mandibular hyperplasia</td>\n      <td>mandibular hyperplasia</td>\n      <td>Major anomalies of jaw size</td>\n    </tr>\n    <tr>\n      <th>36830</th>\n      <td>S59032</td>\n      <td>S</td>\n      <td>S59032S</td>\n      <td>Sltr-haris Type III physl fx lower end ulna, l...</td>\n      <td>salter-harris type iii physeal fracture of low...</td>\n      <td>Salter-Harris Type III physeal fracture of low...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nX = new_df[['long_description']]\ny = new_df['prefix_codes']\n\n\nencoder = LabelEncoder()\ny = encoder.fit_transform(y)\n\n# Split the dataset into a train and temporary set\nX_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=23)\n\n# Split the temporary set into validation and train set\nX_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=23)\n\nprint(X_train.shape, X_val.shape, X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-06-01T16:44:33.755662Z","iopub.execute_input":"2023-06-01T16:44:33.756049Z","iopub.status.idle":"2023-06-01T16:44:33.866972Z","shell.execute_reply.started":"2023-06-01T16:44:33.756021Z","shell.execute_reply":"2023-06-01T16:44:33.865942Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"(43021, 1) (14341, 1) (14341, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\nclass CustomDataset(Dataset):\n    def __init__(self, X, y, tokenizer, max_length):\n        self.data = []\n        for i in zip(X, y):\n            text, target = i[0], i[1]\n            self.data.append((text, int(target)))\n            self.tokenizer = tokenizer\n            self.max_length = max_length\n            \n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        text, target = self.data[idx]\n        inputs = self.tokenizer(text, padding='max_length', truncation=True, \n                                max_length=self.max_length, return_tensors='pt')\n        inputs_ids = inputs['input_ids'].squeeze(0)\n        attention_masks = inputs['attention_mask'].squeeze(0)\n        return inputs_ids, attention_masks, target ","metadata":{"execution":{"iopub.status.busy":"2023-06-01T16:46:52.763968Z","iopub.execute_input":"2023-06-01T16:46:52.764750Z","iopub.status.idle":"2023-06-01T16:46:52.773715Z","shell.execute_reply.started":"2023-06-01T16:46:52.764711Z","shell.execute_reply":"2023-06-01T16:46:52.772591Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"sentences = new_df[\"long_description\"]\nsentences_tokenized = [tokenizer.tokenize(sentence) for sentence in sentences ]\n\nprint('Max sentence length: ', max([len(sen) for sen in sentences_tokenized]))\nmax_len = max([len(sen) for sen in sentences_tokenized])","metadata":{"execution":{"iopub.status.busy":"2023-06-01T16:47:03.081961Z","iopub.execute_input":"2023-06-01T16:47:03.082335Z","iopub.status.idle":"2023-06-01T16:47:47.263352Z","shell.execute_reply.started":"2023-06-01T16:47:03.082309Z","shell.execute_reply":"2023-06-01T16:47:47.262156Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Max sentence length:  43\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset = CustomDataset(X_train, y_train, tokenizer, max_len)\nval_dataset = CustomDataset(X_val, y_val, tokenizer, max_len)\n\nbatch_size = 4\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-01T16:47:47.265420Z","iopub.execute_input":"2023-06-01T16:47:47.265994Z","iopub.status.idle":"2023-06-01T16:47:47.272665Z","shell.execute_reply.started":"2023-06-01T16:47:47.265962Z","shell.execute_reply":"2023-06-01T16:47:47.271591Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}